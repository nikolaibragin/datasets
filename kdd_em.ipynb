{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de9e3e-45d0-4df8-8277-147cc53a14dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading data from 'ad_features_all_gt1_14_08.parquet'...\n",
      "   The file contains 3,922,520 objects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Parquet batches: 100%|████████████████████| 1/1 [00:07<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining batches...\n",
      " Data loaded. Matrix size: (3922520, 51)\n",
      "\n",
      "Step 2: Imputing missing values (NaN) using IterativeImputer (EM-like approach)...\n",
      "   Found 102,253,461 missing values (NaN) to impute.\n",
      "   Starting imputation process (this may take a while)...\n",
      "[IterativeImputer] Completing matrix with shape (3922520, 51)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 320.40\n",
      "[IterativeImputer] Change: 69401.0625, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 624.70\n",
      "[IterativeImputer] Change: 214316.40625, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 938.77\n",
      "[IterativeImputer] Change: 291801.96875, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 1251.22\n",
      "[IterativeImputer] Change: 1085525.625, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 1566.48\n",
      "[IterativeImputer] Change: 17204780.0, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 1932.89\n",
      "[IterativeImputer] Change: 9433064.0, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 2301.14\n",
      "[IterativeImputer] Change: 2937353.5, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 2698.15\n",
      "[IterativeImputer] Change: 5019415.5, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 3138.40\n",
      "[IterativeImputer] Change: 1595203.375, scaled tolerance: 52.08502960205078 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 3505.13\n",
      "[IterativeImputer] Change: 2193193.25, scaled tolerance: 52.08502960205078 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home/projects/fink/.venv/lib/python3.13/site-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Imputation complete in 3513.21 seconds.\n",
      "   Matrix size remains: (3922520, 51)\n",
      "\n",
      "Step 3: Normalizing data (Standardization)...\n",
      "   Data successfully normalized.\n",
      "\n",
      "Step 5: Finding target objects and preparing query vectors...\n",
      "   Found 10 target objects. Preparing to search.\n",
      "\n",
      "Step 4: Building KD-Tree...\n",
      "   (This may take a few minutes depending on data size and CPU power)\n",
      "   Tree built in 31.69 sec.\n",
      "\n",
      "   Performing batch search for 10 objects...\n",
      "   Search completed in 0.2278 sec.\n",
      "\n",
      "================================================================================\n",
      "NEAREST NEIGHBOR SEARCH RESULTS (KD-Tree, data imputed with IterativeImputer)\n",
      "================================================================================\n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF22abegjtx ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF18abeocrt         0.2748              \n",
      "2     ZTF18abncwah         0.2926              \n",
      "3     ZTF22aafpgzk         0.2953              \n",
      "4     ZTF18abdlrnf         0.3467              \n",
      "5     ZTF22aanaxtu         0.3958              \n",
      "6     ZTF18abesbws         0.4433              \n",
      "7     ZTF19aaocieg         0.4477              \n",
      "8     ZTF22abbsbma         0.4669              \n",
      "9     ZTF18abtcryi         0.4672              \n",
      "10    ZTF19abjfjbi         0.4675              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF22abkfhua ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF22abliiap         0.0602              \n",
      "2     ZTF21acjfofl         0.0799              \n",
      "3     ZTF18absmdbg         0.0896              \n",
      "4     ZTF24abgdcyd         0.0915              \n",
      "5     ZTF18acewppy         0.0977              \n",
      "6     ZTF18abvppio         0.0997              \n",
      "7     ZTF20acqutxz         0.1068              \n",
      "8     ZTF18abnvdqk         0.1081              \n",
      "9     ZTF19aalawog         0.1089              \n",
      "10    ZTF18abmagxv         0.1128              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF23aapyidj ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF18aaajboh         0.9879              \n",
      "2     ZTF23aamajfh         1.0426              \n",
      "3     ZTF18abjzard         1.0975              \n",
      "4     ZTF19acpnkei         1.0989              \n",
      "5     ZTF18achejbg         1.1640              \n",
      "6     ZTF23abflamz         1.1813              \n",
      "7     ZTF20abxuxhg         1.1858              \n",
      "8     ZTF18abikocm         1.2051              \n",
      "9     ZTF18aasxswb         1.2257              \n",
      "10    ZTF20aaoqodw         1.2285              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF23abohtqf ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF23absdgxp         1.4896              \n",
      "2     ZTF18abpgkfi         1.5044              \n",
      "3     ZTF24aajhtjl         1.5341              \n",
      "4     ZTF18abuagmy         1.6040              \n",
      "5     ZTF19aalaqug         1.6298              \n",
      "6     ZTF18abcbdkl         1.6665              \n",
      "7     ZTF23aafmydw         1.6749              \n",
      "8     ZTF22aakytij         1.6879              \n",
      "9     ZTF19aapfdcz         1.6956              \n",
      "10    ZTF20abzjcze         1.7706              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF24aaecooj ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF20abgvdqp         0.2064              \n",
      "2     ZTF18abthdgk         0.3312              \n",
      "3     ZTF18abjlyju         0.3332              \n",
      "4     ZTF23abpqoeq         0.3504              \n",
      "5     ZTF19aalzdfm         0.3508              \n",
      "6     ZTF20aavdcpm         0.3762              \n",
      "7     ZTF19abqezgq         0.3764              \n",
      "8     ZTF20aaxyqti         0.3855              \n",
      "9     ZTF18abgjymw         0.3860              \n",
      "10    ZTF23aagmehg         0.3924              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF24aajvvhj ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF17aabvslm         2.8521              \n",
      "2     ZTF24aagtlpi         2.8722              \n",
      "3     ZTF23aajdtbw         2.9722              \n",
      "4     ZTF23abccwbx         3.3155              \n",
      "5     ZTF25aajdsux         3.3516              \n",
      "6     ZTF25aaipwdb         3.4149              \n",
      "7     ZTF19aaeqxfm         3.4829              \n",
      "8     ZTF25aalzhzu         3.5030              \n",
      "9     ZTF19aaycijr         3.5240              \n",
      "10    ZTF23aamlodm         3.5768              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF24aakaiha ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF24aaxfupa         0.6326              \n",
      "2     ZTF21absqexg         0.7207              \n",
      "3     ZTF18acacnlg         0.7785              \n",
      "4     ZTF18aawmbcz         0.7963              \n",
      "5     ZTF18abdlibj         0.8058              \n",
      "6     ZTF18aabpxdx         0.8156              \n",
      "7     ZTF19abdotob         0.8361              \n",
      "8     ZTF18abtlzxw         0.8470              \n",
      "9     ZTF18adjoxoe         0.9029              \n",
      "10    ZTF19aasgxva         0.9223              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF24aamfius ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF20abmkioz         1.9030              \n",
      "2     ZTF24aaqtylr         1.9876              \n",
      "3     ZTF24abwdeuc         2.0301              \n",
      "4     ZTF21aabhtpt         2.0713              \n",
      "5     ZTF20abnoawq         2.0743              \n",
      "6     ZTF19abdkdrj         2.1037              \n",
      "7     ZTF21aacvesu         2.1771              \n",
      "8     ZTF25aadyjle         2.3951              \n",
      "9     ZTF24aafbbfy         2.4475              \n",
      "10    ZTF19abdxxhc         2.4490              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF24aatxshz ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF18acejdhv         0.6144              \n",
      "2     ZTF21aalgusi         0.6805              \n",
      "3     ZTF22abggitl         0.7008              \n",
      "4     ZTF23abeorpc         0.9788              \n",
      "5     ZTF20acwysdj         1.0248              \n",
      "6     ZTF22abegtcu         1.0497              \n",
      "7     ZTF21aatwsrp         1.0716              \n",
      "8     ZTF19abagbkg         1.1029              \n",
      "9     ZTF24aabuxqg         1.1095              \n",
      "10    ZTF22aagcrho         1.1121              \n",
      "\n",
      "--- 10 nearest unique neighbors for: ZTF24abfaake ---\n",
      "\n",
      "Rank  ObjectID             L2 Distance         \n",
      "--------------------------------------------------\n",
      "1     ZTF21abtnxfy         1.8697              \n",
      "2     ZTF24abimtdl         1.9817              \n",
      "3     ZTF20aaozrmv         2.0337              \n",
      "4     ZTF23abnkdcu         2.1282              \n",
      "5     ZTF18abdqopl         2.1516              \n",
      "6     ZTF24abyjsst         2.1703              \n",
      "7     ZTF19aawtgsz         2.2998              \n",
      "8     ZTF23abbnkiw         2.3386              \n",
      "9     ZTF21abryplq         2.3427              \n",
      "10    ZTF18aaijwki         2.3581              \n",
      "\n",
      "================================================================================\n",
      "Saving results to file 'tde_nearest_neighbors_results_all_with_em_imputation.json'...\n",
      "Results successfully saved.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.neighbors import KDTree\n",
    "# --- ИСПРАВЛЕНИЕ: Сначала включаем экспериментальную функцию ---\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# --- Затем импортируем ее ---\n",
    "from sklearn.impute import IterativeImputer\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = \"ad_features_all_gt1_14_08.parquet\"\n",
    "OUTPUT_FILE = \"tde_nearest_neighbors_results_all_with_em_imputation.json\"\n",
    "\n",
    "# Target Object IDs for different categories can be uncommented as needed.\n",
    "# TDE\n",
    "TARGET_OBJECT_IDS = ['ZTF22abegjtx', 'ZTF22abkfhua', 'ZTF23aapyidj', 'ZTF23abohtqf', 'ZTF24aaecooj', 'ZTF24aajvvhj', 'ZTF24aakaiha', 'ZTF24aamfius', 'ZTF24aatxshz', 'ZTF24abfaake']\n",
    "# SLNS\n",
    "#TARGET_OBJECT_IDS = ['ZTF25aanxtou', 'ZTF25aaixrfr', 'ZTF24abvftmi', 'ZTF24abdhylt', 'ZTF24aaysowl', 'ZTF23aboebgh', 'ZTF23abcvbqq', 'ZTF23aanptpp', 'ZTF22abcvfgs', 'ZTF21ackxdos']\n",
    "# CV\n",
    "#TARGET_OBJECT_IDS = ['ZTF18abjcuhv', 'ZTF18aaudzmj', 'ZTF19aagxcga', 'ZTF18aaashju', 'ZTF21aagtpxy', 'ZTF18aboacjl', 'ZTF17aaantxj', 'ZTF18aajwgru', 'ZTF18abigrzf', 'ZTF18adibadz']\n",
    "\n",
    "NUM_NEIGHBORS = 10\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'mean_g', 'weighted_mean_g',\n",
    "    'standard_deviation_g', 'median_g', 'amplitude_g', 'beyond_1_std_g',\n",
    "    'cusum_g', 'inter_percentile_range_10_g', 'kurtosis_g', 'linear_trend_g',\n",
    "    'linear_trend_sigma_g', 'linear_trend_noise_g', 'linear_fit_slope_g',\n",
    "    'linear_fit_slope_sigma_g', 'linear_fit_reduced_chi2_g',\n",
    "    'magnitude_percentage_ratio_40_5_g', 'magnitude_percentage_ratio_20_10_g',\n",
    "    'maximum_slope_g', 'median_absolute_deviation_g',\n",
    "    'median_buffer_range_percentage_10_g', 'percent_amplitude_g',\n",
    "    'anderson_darling_normal_g', 'chi2_g', 'skew_g', 'stetson_K_g', 'mean_r',\n",
    "    'weighted_mean_r', 'standard_deviation_r', 'median_r', 'amplitude_r',\n",
    "    'beyond_1_std_r', 'cusum_r', 'inter_percentile_range_10_r', 'kurtosis_r',\n",
    "    'linear_trend_r', 'linear_trend_sigma_r', 'linear_trend_noise_r',\n",
    "    'linear_fit_slope_r', 'linear_fit_slope_sigma_r', 'linear_fit_reduced_chi2_r',\n",
    "    'magnitude_percentage_ratio_40_5_r', 'magnitude_percentage_ratio_20_10_r',\n",
    "    'maximum_slope_r', 'median_absolute_deviation_r',\n",
    "    'median_buffer_range_percentage_10_r', 'percent_amplitude_r',\n",
    "    'anderson_darling_normal_r', 'chi2_r', 'skew_r', 'stetson_K_r', 'distnr'\n",
    "]\n",
    "\n",
    "def process_data_from_parquet(filepath):\n",
    "    \"\"\"\n",
    "    Loads data from a Parquet file, imputes missing values using an EM-like\n",
    "    algorithm (IterativeImputer), and then normalizes the data.\n",
    "    \"\"\"\n",
    "    print(f\"Step 1: Reading data from '{filepath}'...\")\n",
    "    try:\n",
    "        pq_file = pq.ParquetFile(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not open Parquet file '{filepath}'. {e}\"); return None, None\n",
    "\n",
    "    num_objects_initial = pq_file.metadata.num_rows\n",
    "    print(f\"   The file contains {num_objects_initial:,} objects.\")\n",
    "\n",
    "    object_ids_list, feature_batches = [], []\n",
    "    columns_to_read = ['objectId'] + FEATURE_NAMES\n",
    "\n",
    "    for i in tqdm(range(pq_file.num_row_groups), desc=\"Reading Parquet batches\"):\n",
    "        batch = pq_file.read_row_group(i, columns=columns_to_read)\n",
    "        object_ids_list.extend(batch.column('objectId').to_pylist())\n",
    "        feature_batches.append(np.column_stack([batch.column(name).to_numpy() for name in FEATURE_NAMES]))\n",
    "\n",
    "    print(\"Combining batches...\")\n",
    "    data_matrix = np.vstack(feature_batches).astype('float32')\n",
    "    object_ids = np.array(object_ids_list)\n",
    "    print(f\" Data loaded. Matrix size: {data_matrix.shape}\")\n",
    "\n",
    "    print(\"\\nStep 2: Imputing missing values (NaN) using IterativeImputer (EM-like approach)...\")\n",
    "    total_nan_values = np.isnan(data_matrix).sum()\n",
    "\n",
    "    if total_nan_values > 0:\n",
    "        print(f\"   Found {total_nan_values:,} missing values (NaN) to impute.\")\n",
    "        \n",
    "        # IterativeImputer is a multivariate imputer that models each feature with\n",
    "        # missing values as a function of other features. It's a robust method\n",
    "        # similar in spirit to the EM algorithm for this kind of task.\n",
    "        imputer = IterativeImputer(max_iter=10, random_state=42, verbose=2)\n",
    "        \n",
    "        print(\"   Starting imputation process (this may take a while)...\")\n",
    "        start_time = time.time()\n",
    "        data_matrix = imputer.fit_transform(data_matrix)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"   Imputation complete in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"   Matrix size remains: {data_matrix.shape}\")\n",
    "    else:\n",
    "        print(\"   No missing values (NaN) found. Skipping imputation.\")\n",
    "\n",
    "    print(\"\\nStep 3: Normalizing data (Standardization)...\")\n",
    "    mean = np.mean(data_matrix, axis=0, dtype='float32')\n",
    "    std = np.std(data_matrix, axis=0, dtype='float32')\n",
    "    std[std == 0] = 1.0  # Avoid division by zero for constant features\n",
    "    data_matrix = (data_matrix - mean) / std\n",
    "    print(\"   Data successfully normalized.\")\n",
    "\n",
    "    return object_ids.tolist(), data_matrix\n",
    "\n",
    "def find_neighbors_kdtree_batch(data, query_vectors, k_candidates):\n",
    "    \"\"\"\n",
    "    Builds a KD-Tree on the CPU and performs a batch search.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 4: Building KD-Tree...\")\n",
    "    print(\"   (This may take a few minutes depending on data size and CPU power)\")\n",
    "    start_time = time.time()\n",
    "    kdt = KDTree(data, leaf_size=40, metric='euclidean')\n",
    "    end_time = time.time()\n",
    "    print(f\"   Tree built in {end_time - start_time:.2f} sec.\")\n",
    "\n",
    "    print(f\"\\n   Performing batch search for {len(query_vectors)} objects...\")\n",
    "    start_time = time.time()\n",
    "    distances, indices = kdt.query(query_vectors, k=k_candidates)\n",
    "    end_time = time.time()\n",
    "    print(f\"   Search completed in {end_time - start_time:.4f} sec.\")\n",
    "\n",
    "    return distances, indices\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"ERROR: Data file '{DATA_FILE}' not found.\")\n",
    "        exit()\n",
    "\n",
    "    all_ids, norm_data = process_data_from_parquet(DATA_FILE)\n",
    "    if all_ids is None:\n",
    "        exit()\n",
    "\n",
    "    print(\"\\nStep 5: Finding target objects and preparing query vectors...\")\n",
    "    id_to_idx = {oid: i for i, oid in enumerate(all_ids)}\n",
    "\n",
    "    query_vectors_list, found_target_ids = [], []\n",
    "    for target_id in TARGET_OBJECT_IDS:\n",
    "        if target_id in id_to_idx:\n",
    "            query_vectors_list.append(norm_data[id_to_idx[target_id]])\n",
    "            found_target_ids.append(target_id)\n",
    "        else:\n",
    "            # This is less likely to happen now, but good to keep as a safeguard\n",
    "            print(f\"   Warning: Target object '{target_id}' not found in the original Parquet file and will be skipped.\")\n",
    "\n",
    "    if not found_target_ids:\n",
    "        print(\"ERROR: None of the target objects were found in the data. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    query_vectors = np.array(query_vectors_list, dtype='float32')\n",
    "    print(f\"   Found {len(found_target_ids)} target objects. Preparing to search.\")\n",
    "\n",
    "    distances_raw, indices_raw = find_neighbors_kdtree_batch(norm_data, query_vectors, NUM_NEIGHBORS + 1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NEAREST NEIGHBOR SEARCH RESULTS (KD-Tree, data imputed with IterativeImputer)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_to_save = {}\n",
    "\n",
    "    for i, target_id in enumerate(found_target_ids):\n",
    "        final_neighbors_for_print = []\n",
    "        final_neighbor_ids_for_json = []\n",
    "        # The target object itself will be the first result (distance 0), so we find its unique neighbors.\n",
    "        seen_ids = {target_id}\n",
    "\n",
    "        distances_for_target, indices_for_target = distances_raw[i], indices_raw[i]\n",
    "\n",
    "        for j in range(len(indices_for_target)):\n",
    "            if len(final_neighbor_ids_for_json) >= NUM_NEIGHBORS:\n",
    "                break\n",
    "\n",
    "            neighbor_id = all_ids[indices_for_target[j]]\n",
    "            if neighbor_id in seen_ids:\n",
    "                continue\n",
    "            \n",
    "            final_neighbor_ids_for_json.append(neighbor_id)\n",
    "            \n",
    "            neighbor_dist = distances_for_target[j]\n",
    "            final_neighbors_for_print.append({'id': neighbor_id, 'distance': neighbor_dist})\n",
    "            \n",
    "            seen_ids.add(neighbor_id)\n",
    "        \n",
    "        results_to_save[target_id] = final_neighbor_ids_for_json\n",
    "\n",
    "        print(f\"\\n--- {NUM_NEIGHBORS} nearest unique neighbors for: {target_id} ---\\n\")\n",
    "        print(f\"{'Rank':<5} {'ObjectID':<20} {'L2 Distance':<20}\")\n",
    "        print(\"-\"*50)\n",
    "        for rank, neighbor in enumerate(final_neighbors_for_print, 1):\n",
    "            print(f\"{rank:<5} {neighbor['id']:<20} {neighbor['distance']:<20.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Saving results to file '{OUTPUT_FILE}'...\")\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_to_save, f, indent=4)\n",
    "        print(f\"Results successfully saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save the file. {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f685d80-489a-4cd0-b4bb-76f0d4289c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
