{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8751f3-5a20-48f9-bd3a-79015285e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'ad_features_all_gt1_14_08.parquet'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batches: 100%|████████████████████████████| 1/1 [00:07<00:00,  7.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded. Shape: (3922520, 51)\n",
      "\n",
      "Selecting 500 random target objects (skipping rows with NaN)...\n",
      "Selected 500 targets.\n",
      "\n",
      "--- Processing Mode: drop_nan ---\n",
      "   Rows after cleaning NaNs: 279,763\n",
      "   Normalizing...\n",
      "   Building KD-Tree...\n",
      "   Tree built in 1.36 sec.\n",
      "   Searching neighbors for 500 targets...\n",
      "Saved results to kurtosis_r_without_median.json\n",
      "\n",
      "--- Processing Mode: fill_median ---\n",
      "   Filling NaN in 'kurtosis_r' with median: -0.1591\n",
      "   Filled 3221619 values.\n",
      "   Rows after cleaning NaNs: 279,763\n",
      "   Normalizing...\n",
      "   Building KD-Tree...\n",
      "   Tree built in 1.28 sec.\n",
      "   Searching neighbors for 500 targets...\n",
      "Saved results to kurtosis_r_with_median.json\n",
      "\n",
      "Comparing results and writing report to comparison_report.txt...\n",
      "Report generated. Average overlap: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.neighbors import KDTree\n",
    "import json\n",
    "import random\n",
    "\n",
    "DATA_FILE = \"ad_features_all_gt1_14_08.parquet\"\n",
    "FILE_WITHOUT_MEDIAN = \"kurtosis_r_without_median.json\"\n",
    "FILE_WITH_MEDIAN = \"kurtosis_r_with_median.json\"\n",
    "REPORT_FILE = \"comparison_report.txt\"\n",
    "\n",
    "NUM_NEIGHBORS = 10\n",
    "NUM_RANDOM_TARGETS = 500\n",
    "TARGET_FEATURE = 'kurtosis_r'\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'mean_g', 'weighted_mean_g',\n",
    "    'standard_deviation_g', 'median_g', 'amplitude_g', 'beyond_1_std_g',\n",
    "    'cusum_g', 'inter_percentile_range_10_g', 'kurtosis_g', 'linear_trend_g',\n",
    "    'linear_trend_sigma_g', 'linear_trend_noise_g', 'linear_fit_slope_g',\n",
    "    'linear_fit_slope_sigma_g', 'linear_fit_reduced_chi2_g',\n",
    "    'magnitude_percentage_ratio_40_5_g', 'magnitude_percentage_ratio_20_10_g',\n",
    "    'maximum_slope_g', 'median_absolute_deviation_g',\n",
    "    'median_buffer_range_percentage_10_g', 'percent_amplitude_g',\n",
    "    'anderson_darling_normal_g', 'chi2_g', 'skew_g', 'stetson_K_g', 'mean_r',\n",
    "    'weighted_mean_r', 'standard_deviation_r', 'median_r', 'amplitude_r',\n",
    "    'beyond_1_std_r', 'cusum_r', 'inter_percentile_range_10_r', 'kurtosis_r',\n",
    "    'linear_trend_r', 'linear_trend_sigma_r', 'linear_trend_noise_r',\n",
    "    'linear_fit_slope_r', 'linear_fit_slope_sigma_r', 'linear_fit_reduced_chi2_r',\n",
    "    'magnitude_percentage_ratio_40_5_r', 'magnitude_percentage_ratio_20_10_r',\n",
    "    'maximum_slope_r', 'median_absolute_deviation_r',\n",
    "    'median_buffer_range_percentage_10_r', 'percent_amplitude_r',\n",
    "    'anderson_darling_normal_r', 'chi2_r', 'skew_r', 'stetson_K_r', 'distnr'\n",
    "]\n",
    "\n",
    "def load_raw_data(filepath):\n",
    "    \"\"\"Loads raw data from Parquet without dropping NaNs immediately.\"\"\"\n",
    "    print(f\"Reading data from '{filepath}'...\")\n",
    "    try:\n",
    "        pq_file = pq.ParquetFile(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not open Parquet file '{filepath}'. {e}\"); return None, None\n",
    "\n",
    "    object_ids_list, feature_batches = [], []\n",
    "    columns_to_read = ['objectId'] + FEATURE_NAMES\n",
    "\n",
    "    for i in tqdm(range(pq_file.num_row_groups), desc=\"Reading batches\"):\n",
    "        batch = pq_file.read_row_group(i, columns=columns_to_read)\n",
    "        object_ids_list.extend(batch.column('objectId').to_pylist())\n",
    "        feature_batches.append(np.column_stack([batch.column(name).to_numpy() for name in FEATURE_NAMES]))\n",
    "\n",
    "    data_matrix = np.vstack(feature_batches).astype('float32')\n",
    "    object_ids = np.array(object_ids_list)\n",
    "    print(f\"Raw data loaded. Shape: {data_matrix.shape}\")\n",
    "    return object_ids, data_matrix\n",
    "\n",
    "def process_and_search(raw_ids, raw_data, target_ids, mode='drop_nan'):\n",
    "    \"\"\"\n",
    "    Processes data (normalization, cleaning/imputing), builds KDTree, and searches.\n",
    "    mode: 'drop_nan' (standard) or 'fill_median' (impute kurtosis_r).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Mode: {mode} ---\")\n",
    "    \n",
    "\n",
    "    data_processed = raw_data.copy()\n",
    "    \n",
    "    if mode == 'fill_median':\n",
    "        if TARGET_FEATURE in FEATURE_NAMES:\n",
    "            idx = FEATURE_NAMES.index(TARGET_FEATURE)\n",
    "\n",
    "            median_val = np.nanmedian(data_processed[:, idx])\n",
    "            print(f\"   Filling NaN in '{TARGET_FEATURE}' with median: {median_val:.4f}\")\n",
    "\n",
    "            col_data = data_processed[:, idx]\n",
    "            mask_nan = np.isnan(col_data)\n",
    "            data_processed[mask_nan, idx] = median_val\n",
    "            print(f\"   Filled {np.sum(mask_nan)} values.\")\n",
    "        else:\n",
    "            print(f\"   Warning: Feature {TARGET_FEATURE} not found.\")\n",
    "\n",
    "    nan_mask = np.isnan(data_processed).any(axis=1)\n",
    "    clean_data = data_processed[~nan_mask]\n",
    "    clean_ids = raw_ids[~nan_mask]\n",
    "    print(f\"   Rows after cleaning NaNs: {clean_data.shape[0]:,}\")\n",
    "\n",
    "\n",
    "    print(\"   Normalizing...\")\n",
    "    mean = np.mean(clean_data, axis=0, dtype='float32')\n",
    "    std = np.std(clean_data, axis=0, dtype='float32')\n",
    "    std[std == 0] = 1.0\n",
    "    norm_data = (clean_data - mean) / std\n",
    "\n",
    "\n",
    "    print(\"   Building KD-Tree...\")\n",
    "    start_time = time.time()\n",
    "    kdt = KDTree(norm_data, leaf_size=40, metric='euclidean')\n",
    "    print(f\"   Tree built in {time.time() - start_time:.2f} sec.\")\n",
    "\n",
    "    # 5. Prepare Query Vectors\n",
    "    # Map IDs to new indices in the cleaned/normalized array\n",
    "    id_to_idx = {oid: i for i, oid in enumerate(clean_ids)}\n",
    "    \n",
    "    query_vectors = []\n",
    "    valid_target_ids = []\n",
    "    \n",
    "    for tid in target_ids:\n",
    "        if tid in id_to_idx:\n",
    "            query_vectors.append(norm_data[id_to_idx[tid]])\n",
    "            valid_target_ids.append(tid)\n",
    "\n",
    "            \n",
    "    query_vectors = np.array(query_vectors, dtype='float32')\n",
    "\n",
    "    print(f\"   Searching neighbors for {len(query_vectors)} targets...\")\n",
    "\n",
    "    distances, indices = kdt.query(query_vectors, k=NUM_NEIGHBORS + 1)\n",
    "\n",
    "    # 7. Format Results\n",
    "    results = {}\n",
    "    for i, tid in enumerate(valid_target_ids):\n",
    "        neighbor_idxs = indices[i]\n",
    "        # neighbors_list = []\n",
    "        neighbor_ids_only = []\n",
    "        \n",
    "        count = 0\n",
    "        for n_idx in neighbor_idxs:\n",
    "            n_id = clean_ids[n_idx]\n",
    "            if n_id == tid:\n",
    "                continue # Skip self\n",
    "            if count >= NUM_NEIGHBORS:\n",
    "                break\n",
    "            neighbor_ids_only.append(n_id)\n",
    "            count += 1\n",
    "            \n",
    "        results[tid] = neighbor_ids_only\n",
    "        \n",
    "    return results\n",
    "\n",
    "def select_random_targets(ids, data, n=500):\n",
    "    \"\"\"Selects N random objects that define the 'Gold Standard' (must not have NaNs).\"\"\"\n",
    "    print(f\"\\nSelecting {n} random target objects (skipping rows with NaN)...\")\n",
    "\n",
    "    valid_mask = ~np.isnan(data).any(axis=1)\n",
    "    valid_ids = ids[valid_mask]\n",
    "    \n",
    "    if len(valid_ids) < n:\n",
    "        print(f\"Warning: Only {len(valid_ids)} valid objects found. Using all.\")\n",
    "        selected = valid_ids\n",
    "    else:\n",
    "\n",
    "        selected = np.random.choice(valid_ids, n, replace=False)\n",
    "        \n",
    "    print(f\"Selected {len(selected)} targets.\")\n",
    "    return selected.tolist()\n",
    "\n",
    "def compare_and_report(res1, res2, output_txt):\n",
    "    print(f\"\\nComparing results and writing report to {output_txt}...\")\n",
    "    \n",
    "    common_targets = set(res1.keys()).intersection(set(res2.keys()))\n",
    "    total_targets = len(common_targets)\n",
    "    \n",
    "    if total_targets == 0:\n",
    "        print(\"Error: No common targets found between runs.\")\n",
    "        return\n",
    "\n",
    "    total_overlap_percent = 0.0\n",
    "    \n",
    "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"NEAREST NEIGHBOR COMPARISON REPORT\\n\")\n",
    "        f.write(\"==================================\\n\")\n",
    "        f.write(f\"Total targets compared: {total_targets}\\n\")\n",
    "        f.write(f\"Neighbors per target: {NUM_NEIGHBORS}\\n\")\n",
    "        f.write(f\"Comparison: Drop NaN vs. Fill NaN with Median ({TARGET_FEATURE})\\n\\n\")\n",
    "        f.write(f\"{'ObjectID':<20} | {'Overlap (Cnt)':<15} | {'Overlap (%)':<15}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        for tid in common_targets:\n",
    "            set1 = set(res1[tid])\n",
    "            set2 = set(res2[tid])\n",
    "            \n",
    "            intersection = set1.intersection(set2)\n",
    "            overlap_count = len(intersection)\n",
    "            overlap_pct = (overlap_count / NUM_NEIGHBORS) * 100\n",
    "            \n",
    "            total_overlap_percent += overlap_pct\n",
    "            \n",
    "            f.write(f\"{tid:<20} | {overlap_count:<15} | {overlap_pct:<15.1f}\\n\")\n",
    "        \n",
    "        avg_overlap = total_overlap_percent / total_targets\n",
    "        \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"AVERAGE OVERLAP PERCENTAGE: {avg_overlap:.2f}%\\n\")\n",
    "        \n",
    "    print(f\"Report generated. Average overlap: {avg_overlap:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"ERROR: Data file '{DATA_FILE}' not found.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    all_ids, raw_data = load_raw_data(DATA_FILE)\n",
    "    if all_ids is None:\n",
    "        exit()\n",
    "\n",
    "\n",
    "    target_objects = select_random_targets(all_ids, raw_data, NUM_RANDOM_TARGETS)\n",
    "\n",
    "    results_without = process_and_search(all_ids, raw_data, target_objects, mode='drop_nan')\n",
    "    \n",
    "\n",
    "    with open(FILE_WITHOUT_MEDIAN, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_without, f, indent=4)\n",
    "    print(f\"Saved results to {FILE_WITHOUT_MEDIAN}\")\n",
    "    results_with = process_and_search(all_ids, raw_data, target_objects, mode='fill_median')\n",
    "\n",
    "    with open(FILE_WITH_MEDIAN, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_with, f, indent=4)\n",
    "    print(f\"Saved results to {FILE_WITH_MEDIAN}\")\n",
    "\n",
    "    compare_and_report(results_without, results_with, REPORT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f55c5-d1de-4adf-8f14-4decffaf1a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'ad_features_all_gt1_14_08.parquet'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batches: 100%|████████████████████████████| 1/1 [00:07<00:00,  7.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded. Shape: (3922520, 51)\n",
      "\n",
      "--- Preparing Dataset: Mode 'drop_nan' ---\n",
      "   Rows valid for tree: 279,763\n",
      "   Normalizing data...\n",
      "   Building KD-Tree...\n",
      "   Tree built in 1.31 sec.\n",
      "\n",
      "--- Preparing Dataset: Mode 'fill_median' ---\n",
      "   [Imputation] Filling NaN in 'kurtosis_r' with median: -0.1591\n",
      "   [Imputation] Filled 3221619 values.\n",
      "   Rows valid for tree: 279,763\n",
      "   Normalizing data...\n",
      "   Building KD-Tree...\n",
      "   Tree built in 1.33 sec.\n",
      "\n",
      "Pool of valid targets (No NaN, nalerthist > 3): 279,763 objects.\n",
      "\n",
      "============================================================\n",
      "STARTING 100 EXPERIMENTS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.neighbors import KDTree\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = \"ad_features_all_gt1_14_08.parquet\"\n",
    "NUM_EXPERIMENTS = 100\n",
    "NUM_TARGETS = 500\n",
    "NUM_NEIGHBORS = 10\n",
    "FILTER_NALERTHIST = 3\n",
    "TARGET_FEATURE = 'kurtosis_r'\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'mean_g', 'weighted_mean_g',\n",
    "    'standard_deviation_g', 'median_g', 'amplitude_g', 'beyond_1_std_g',\n",
    "    'cusum_g', 'inter_percentile_range_10_g', 'kurtosis_g', 'linear_trend_g',\n",
    "    'linear_trend_sigma_g', 'linear_trend_noise_g', 'linear_fit_slope_g',\n",
    "    'linear_fit_slope_sigma_g', 'linear_fit_reduced_chi2_g',\n",
    "    'magnitude_percentage_ratio_40_5_g', 'magnitude_percentage_ratio_20_10_g',\n",
    "    'maximum_slope_g', 'median_absolute_deviation_g',\n",
    "    'median_buffer_range_percentage_10_g', 'percent_amplitude_g',\n",
    "    'anderson_darling_normal_g', 'chi2_g', 'skew_g', 'stetson_K_g', 'mean_r',\n",
    "    'weighted_mean_r', 'standard_deviation_r', 'median_r', 'amplitude_r',\n",
    "    'beyond_1_std_r', 'cusum_r', 'inter_percentile_range_10_r', 'kurtosis_r',\n",
    "    'linear_trend_r', 'linear_trend_sigma_r', 'linear_trend_noise_r',\n",
    "    'linear_fit_slope_r', 'linear_fit_slope_sigma_r', 'linear_fit_reduced_chi2_r',\n",
    "    'magnitude_percentage_ratio_40_5_r', 'magnitude_percentage_ratio_20_10_r',\n",
    "    'maximum_slope_r', 'median_absolute_deviation_r',\n",
    "    'median_buffer_range_percentage_10_r', 'percent_amplitude_r',\n",
    "    'anderson_darling_normal_r', 'chi2_r', 'skew_r', 'stetson_K_r', 'distnr'\n",
    "]\n",
    "\n",
    "def load_raw_data(filepath):\n",
    "    \"\"\"Loads raw data including nalerthist.\"\"\"\n",
    "    print(f\"Reading data from '{filepath}'...\")\n",
    "    try:\n",
    "        pq_file = pq.ParquetFile(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not open Parquet file '{filepath}'. {e}\"); return None, None, None\n",
    "\n",
    "    # Include nalerthist in reading, but separate it later\n",
    "    columns_to_read = ['objectId', 'nalerthist'] + FEATURE_NAMES\n",
    "    \n",
    "    object_ids_list = []\n",
    "    nalerthist_list = []\n",
    "    feature_batches = []\n",
    "\n",
    "    for i in tqdm(range(pq_file.num_row_groups), desc=\"Reading batches\"):\n",
    "        batch = pq_file.read_row_group(i, columns=columns_to_read)\n",
    "        object_ids_list.extend(batch.column('objectId').to_pylist())\n",
    "        nalerthist_list.extend(batch.column('nalerthist').to_pylist())\n",
    "        feature_batches.append(np.column_stack([batch.column(name).to_numpy() for name in FEATURE_NAMES]))\n",
    "\n",
    "    data_matrix = np.vstack(feature_batches).astype('float32')\n",
    "    object_ids = np.array(object_ids_list)\n",
    "    nalerthist_arr = np.array(nalerthist_list) # Keep as array for filtering\n",
    "    \n",
    "    print(f\"Raw data loaded. Shape: {data_matrix.shape}\")\n",
    "    return object_ids, nalerthist_arr, data_matrix\n",
    "\n",
    "def prepare_dataset_and_tree(raw_ids, raw_nalert, raw_data, mode='drop_nan'):\n",
    "    \"\"\"\n",
    "    Prepares the dataset (cleaning/imputing/normalizing) and builds the KD-Tree.\n",
    "    Returns: \n",
    "        norm_data (matrix), \n",
    "        clean_ids (array), \n",
    "        id_to_idx (dict), \n",
    "        kdtree (sklearn object)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Preparing Dataset: Mode '{mode}' ---\")\n",
    "    data_processed = raw_data.copy()\n",
    "    \n",
    "\n",
    "    if mode == 'fill_median':\n",
    "        if TARGET_FEATURE in FEATURE_NAMES:\n",
    "            idx = FEATURE_NAMES.index(TARGET_FEATURE)\n",
    "            median_val = np.nanmedian(data_processed[:, idx])\n",
    "            print(f\"   [Imputation] Filling NaN in '{TARGET_FEATURE}' with median: {median_val:.4f}\")\n",
    "            \n",
    "            col_data = data_processed[:, idx]\n",
    "            mask_nan = np.isnan(col_data)\n",
    "            data_processed[mask_nan, idx] = median_val\n",
    "            print(f\"   [Imputation] Filled {np.sum(mask_nan)} values.\")\n",
    "\n",
    "    nan_mask = np.isnan(data_processed).any(axis=1)\n",
    "    clean_data = data_processed[~nan_mask]\n",
    "    clean_ids = raw_ids[~nan_mask]\n",
    "\n",
    "    clean_nalert = raw_nalert[~nan_mask] \n",
    "\n",
    "    print(f\"   Rows valid for tree: {clean_data.shape[0]:,}\")\n",
    "\n",
    "\n",
    "    print(\"   Normalizing data...\")\n",
    "    mean = np.mean(clean_data, axis=0, dtype='float32')\n",
    "    std = np.std(clean_data, axis=0, dtype='float32')\n",
    "    std[std == 0] = 1.0\n",
    "    norm_data = (clean_data - mean) / std\n",
    "\n",
    "    print(\"   Building KD-Tree...\")\n",
    "    t0 = time.time()\n",
    "    kdt = KDTree(norm_data, leaf_size=40, metric='euclidean')\n",
    "    print(f\"   Tree built in {time.time() - t0:.2f} sec.\")\n",
    "    \n",
    "\n",
    "    id_to_idx = {oid: i for i, oid in enumerate(clean_ids)}\n",
    "    \n",
    "    return norm_data, clean_ids, clean_nalert, id_to_idx, kdt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"ERROR: Data file '{DATA_FILE}' not found.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    all_ids, all_nalert, raw_data = load_raw_data(DATA_FILE)\n",
    "    if all_ids is None: exit()\n",
    "\n",
    "\n",
    "    data_A, ids_A, nalert_A, map_A, tree_A = prepare_dataset_and_tree(\n",
    "        all_ids, all_nalert, raw_data, mode='drop_nan'\n",
    "    )\n",
    "\n",
    "\n",
    "    data_B, ids_B, nalert_B, map_B, tree_B = prepare_dataset_and_tree(\n",
    "        all_ids, all_nalert, raw_data, mode='fill_median'\n",
    "    )\n",
    "\n",
    "    valid_indices_in_A = []\n",
    "    for i, (oid, nal) in enumerate(zip(ids_A, nalert_A)):\n",
    "        if nal > FILTER_NALERTHIST:\n",
    "            valid_indices_in_A.append(i)\n",
    "            \n",
    "    valid_indices_in_A = np.array(valid_indices_in_A)\n",
    "    print(f\"\\nPool of valid targets (No NaN, nalerthist > {FILTER_NALERTHIST}): {len(valid_indices_in_A):,} objects.\")\n",
    "\n",
    "    if len(valid_indices_in_A) < NUM_TARGETS:\n",
    "        print(f\"Error: Not enough objects satisfy the criteria to pick {NUM_TARGETS}.\")\n",
    "        exit()\n",
    "\n",
    "    # 5. Run 100 Experiments\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STARTING {NUM_EXPERIMENTS} EXPERIMENTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    experiment_results = []\n",
    "\n",
    "    for exp_i in range(NUM_EXPERIMENTS):\n",
    "\n",
    "        chosen_indices_A = np.random.choice(valid_indices_in_A, NUM_TARGETS, replace=False)\n",
    "        chosen_ids = ids_A[chosen_indices_A]\n",
    "        \n",
    "\n",
    "        query_vecs_A = data_A[chosen_indices_A]\n",
    "\n",
    "        query_vecs_B_list = []\n",
    "        for tid in chosen_ids:\n",
    "            idx_B = map_B[tid]\n",
    "            query_vecs_B_list.append(data_B[idx_B])\n",
    "        query_vecs_B = np.array(query_vecs_B_list, dtype='float32')\n",
    "\n",
    "        _, idxs_neigh_A = tree_A.query(query_vecs_A, k=NUM_NEIGHBORS + 1)\n",
    "\n",
    "        _, idxs_neigh_B = tree_B.query(query_vecs_B, k=NUM_NEIGHBORS + 1)\n",
    "\n",
    "\n",
    "        total_overlap_pct = 0.0\n",
    "        \n",
    "        for k in range(NUM_TARGETS):\n",
    "            target_id = chosen_ids[k]\n",
    "\n",
    "            neighbors_A = []\n",
    "            raw_idx_A = idxs_neigh_A[k]\n",
    "            for idx in raw_idx_A:\n",
    "                nid = ids_A[idx]\n",
    "                if nid != target_id:\n",
    "                    neighbors_A.append(nid)\n",
    "            neighbors_A = neighbors_A[:NUM_NEIGHBORS] \n",
    "\n",
    "            neighbors_B = []\n",
    "            raw_idx_B = idxs_neigh_B[k]\n",
    "            for idx in raw_idx_B:\n",
    "                nid = ids_B[idx]\n",
    "                if nid != target_id:\n",
    "                    neighbors_B.append(nid)\n",
    "            neighbors_B = neighbors_B[:NUM_NEIGHBORS] \n",
    "            \n",
    "\n",
    "            set_A = set(neighbors_A)\n",
    "            set_B = set(neighbors_B)\n",
    "            intersection = len(set_A.intersection(set_B))\n",
    "            \n",
    "            overlap_pct = (intersection / NUM_NEIGHBORS) * 100\n",
    "            total_overlap_pct += overlap_pct\n",
    "\n",
    "        avg_overlap = total_overlap_pct / NUM_TARGETS\n",
    "        experiment_results.append(avg_overlap)\n",
    "        \n",
    "\n",
    "        if (exp_i + 1) % 10 == 0:\n",
    "            print(f\"Experiment {exp_i+1}/{NUM_EXPERIMENTS}: Average Overlap = {avg_overlap:.2f}%\")\n",
    "\n",
    "    # --- Final Report ---\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULTS (Percentage of Intersection)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Results over {NUM_EXPERIMENTS} runs:\")\n",
    "    print(\"-\" * 20)\n",
    "    for i, res in enumerate(experiment_results):\n",
    "        print(f\"Run {i+1:03d}: {res:.2f}%\")\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "    overall_mean = np.mean(experiment_results)\n",
    "    overall_min = np.min(experiment_results)\n",
    "    overall_max = np.max(experiment_results)\n",
    "    print(f\"\\nGlobal Average Overlap: {overall_mean:.2f}%\")\n",
    "    print(f\"Min Overlap: {overall_min:.2f}%\")\n",
    "    print(f\"Max Overlap: {overall_max:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e079991-2a19-4c09-9543-d06e5bb57ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
