{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8751f3-5a20-48f9-bd3a-79015285e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'ad_features_all_gt1_14_08.parquet'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batches: 100%|████████████████████████████| 1/1 [00:07<00:00,  7.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded. Shape: (3922520, 51)\n",
      "\n",
      "Selecting 500 random target objects (skipping rows with NaN)...\n",
      "Selected 500 targets.\n",
      "\n",
      "--- Processing Mode: drop_nan ---\n",
      "   Rows after cleaning NaNs: 279,763\n",
      "   Normalizing...\n",
      "   Building KD-Tree...\n",
      "   Tree built in 1.36 sec.\n",
      "   Searching neighbors for 500 targets...\n",
      "Saved results to kurtosis_r_without_median.json\n",
      "\n",
      "--- Processing Mode: fill_median ---\n",
      "   Filling NaN in 'kurtosis_r' with median: -0.1591\n",
      "   Filled 3221619 values.\n",
      "   Rows after cleaning NaNs: 279,763\n",
      "   Normalizing...\n",
      "   Building KD-Tree...\n",
      "   Tree built in 1.28 sec.\n",
      "   Searching neighbors for 500 targets...\n",
      "Saved results to kurtosis_r_with_median.json\n",
      "\n",
      "Comparing results and writing report to comparison_report.txt...\n",
      "Report generated. Average overlap: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.neighbors import KDTree\n",
    "import json\n",
    "import random\n",
    "\n",
    "DATA_FILE = \"ad_features_all_gt1_14_08.parquet\"\n",
    "FILE_WITHOUT_MEDIAN = \"kurtosis_r_without_median.json\"\n",
    "FILE_WITH_MEDIAN = \"kurtosis_r_with_median.json\"\n",
    "REPORT_FILE = \"comparison_report.txt\"\n",
    "\n",
    "NUM_NEIGHBORS = 10\n",
    "NUM_RANDOM_TARGETS = 500\n",
    "TARGET_FEATURE = 'kurtosis_r'\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'mean_g', 'weighted_mean_g',\n",
    "    'standard_deviation_g', 'median_g', 'amplitude_g', 'beyond_1_std_g',\n",
    "    'cusum_g', 'inter_percentile_range_10_g', 'kurtosis_g', 'linear_trend_g',\n",
    "    'linear_trend_sigma_g', 'linear_trend_noise_g', 'linear_fit_slope_g',\n",
    "    'linear_fit_slope_sigma_g', 'linear_fit_reduced_chi2_g',\n",
    "    'magnitude_percentage_ratio_40_5_g', 'magnitude_percentage_ratio_20_10_g',\n",
    "    'maximum_slope_g', 'median_absolute_deviation_g',\n",
    "    'median_buffer_range_percentage_10_g', 'percent_amplitude_g',\n",
    "    'anderson_darling_normal_g', 'chi2_g', 'skew_g', 'stetson_K_g', 'mean_r',\n",
    "    'weighted_mean_r', 'standard_deviation_r', 'median_r', 'amplitude_r',\n",
    "    'beyond_1_std_r', 'cusum_r', 'inter_percentile_range_10_r', 'kurtosis_r',\n",
    "    'linear_trend_r', 'linear_trend_sigma_r', 'linear_trend_noise_r',\n",
    "    'linear_fit_slope_r', 'linear_fit_slope_sigma_r', 'linear_fit_reduced_chi2_r',\n",
    "    'magnitude_percentage_ratio_40_5_r', 'magnitude_percentage_ratio_20_10_r',\n",
    "    'maximum_slope_r', 'median_absolute_deviation_r',\n",
    "    'median_buffer_range_percentage_10_r', 'percent_amplitude_r',\n",
    "    'anderson_darling_normal_r', 'chi2_r', 'skew_r', 'stetson_K_r', 'distnr'\n",
    "]\n",
    "\n",
    "def load_raw_data(filepath):\n",
    "    \"\"\"Loads raw data from Parquet without dropping NaNs immediately.\"\"\"\n",
    "    print(f\"Reading data from '{filepath}'...\")\n",
    "    try:\n",
    "        pq_file = pq.ParquetFile(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR: Could not open Parquet file '{filepath}'. {e}\"); return None, None\n",
    "\n",
    "    object_ids_list, feature_batches = [], []\n",
    "    columns_to_read = ['objectId'] + FEATURE_NAMES\n",
    "\n",
    "    for i in tqdm(range(pq_file.num_row_groups), desc=\"Reading batches\"):\n",
    "        batch = pq_file.read_row_group(i, columns=columns_to_read)\n",
    "        object_ids_list.extend(batch.column('objectId').to_pylist())\n",
    "        feature_batches.append(np.column_stack([batch.column(name).to_numpy() for name in FEATURE_NAMES]))\n",
    "\n",
    "    data_matrix = np.vstack(feature_batches).astype('float32')\n",
    "    object_ids = np.array(object_ids_list)\n",
    "    print(f\"Raw data loaded. Shape: {data_matrix.shape}\")\n",
    "    return object_ids, data_matrix\n",
    "\n",
    "def process_and_search(raw_ids, raw_data, target_ids, mode='drop_nan'):\n",
    "    \"\"\"\n",
    "    Processes data (normalization, cleaning/imputing), builds KDTree, and searches.\n",
    "    mode: 'drop_nan' (standard) or 'fill_median' (impute kurtosis_r).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Mode: {mode} ---\")\n",
    "    \n",
    "\n",
    "    data_processed = raw_data.copy()\n",
    "    \n",
    "    if mode == 'fill_median':\n",
    "        if TARGET_FEATURE in FEATURE_NAMES:\n",
    "            idx = FEATURE_NAMES.index(TARGET_FEATURE)\n",
    "\n",
    "            median_val = np.nanmedian(data_processed[:, idx])\n",
    "            print(f\"   Filling NaN in '{TARGET_FEATURE}' with median: {median_val:.4f}\")\n",
    "\n",
    "            col_data = data_processed[:, idx]\n",
    "            mask_nan = np.isnan(col_data)\n",
    "            data_processed[mask_nan, idx] = median_val\n",
    "            print(f\"   Filled {np.sum(mask_nan)} values.\")\n",
    "        else:\n",
    "            print(f\"   Warning: Feature {TARGET_FEATURE} not found.\")\n",
    "\n",
    "    nan_mask = np.isnan(data_processed).any(axis=1)\n",
    "    clean_data = data_processed[~nan_mask]\n",
    "    clean_ids = raw_ids[~nan_mask]\n",
    "    print(f\"   Rows after cleaning NaNs: {clean_data.shape[0]:,}\")\n",
    "\n",
    "\n",
    "    print(\"   Normalizing...\")\n",
    "    mean = np.mean(clean_data, axis=0, dtype='float32')\n",
    "    std = np.std(clean_data, axis=0, dtype='float32')\n",
    "    std[std == 0] = 1.0\n",
    "    norm_data = (clean_data - mean) / std\n",
    "\n",
    "\n",
    "    print(\"   Building KD-Tree...\")\n",
    "    start_time = time.time()\n",
    "    kdt = KDTree(norm_data, leaf_size=40, metric='euclidean')\n",
    "    print(f\"   Tree built in {time.time() - start_time:.2f} sec.\")\n",
    "\n",
    "    # 5. Prepare Query Vectors\n",
    "    # Map IDs to new indices in the cleaned/normalized array\n",
    "    id_to_idx = {oid: i for i, oid in enumerate(clean_ids)}\n",
    "    \n",
    "    query_vectors = []\n",
    "    valid_target_ids = []\n",
    "    \n",
    "    for tid in target_ids:\n",
    "        if tid in id_to_idx:\n",
    "            query_vectors.append(norm_data[id_to_idx[tid]])\n",
    "            valid_target_ids.append(tid)\n",
    "        else:\n",
    "            # Should not happen if we selected targets from clean rows initially, \n",
    "            # but good for safety.\n",
    "            pass\n",
    "            \n",
    "    query_vectors = np.array(query_vectors, dtype='float32')\n",
    "    \n",
    "    # 6. Search\n",
    "    print(f\"   Searching neighbors for {len(query_vectors)} targets...\")\n",
    "    # k + 1 because the point itself is included\n",
    "    distances, indices = kdt.query(query_vectors, k=NUM_NEIGHBORS + 1)\n",
    "\n",
    "    # 7. Format Results\n",
    "    results = {}\n",
    "    for i, tid in enumerate(valid_target_ids):\n",
    "        neighbor_idxs = indices[i]\n",
    "        # neighbors_list = []\n",
    "        neighbor_ids_only = []\n",
    "        \n",
    "        count = 0\n",
    "        for n_idx in neighbor_idxs:\n",
    "            n_id = clean_ids[n_idx]\n",
    "            if n_id == tid:\n",
    "                continue # Skip self\n",
    "            if count >= NUM_NEIGHBORS:\n",
    "                break\n",
    "            neighbor_ids_only.append(n_id)\n",
    "            count += 1\n",
    "            \n",
    "        results[tid] = neighbor_ids_only\n",
    "        \n",
    "    return results\n",
    "\n",
    "def select_random_targets(ids, data, n=500):\n",
    "    \"\"\"Selects N random objects that define the 'Gold Standard' (must not have NaNs).\"\"\"\n",
    "    print(f\"\\nSelecting {n} random target objects (skipping rows with NaN)...\")\n",
    "\n",
    "    valid_mask = ~np.isnan(data).any(axis=1)\n",
    "    valid_ids = ids[valid_mask]\n",
    "    \n",
    "    if len(valid_ids) < n:\n",
    "        print(f\"Warning: Only {len(valid_ids)} valid objects found. Using all.\")\n",
    "        selected = valid_ids\n",
    "    else:\n",
    "\n",
    "        selected = np.random.choice(valid_ids, n, replace=False)\n",
    "        \n",
    "    print(f\"Selected {len(selected)} targets.\")\n",
    "    return selected.tolist()\n",
    "\n",
    "def compare_and_report(res1, res2, output_txt):\n",
    "    print(f\"\\nComparing results and writing report to {output_txt}...\")\n",
    "    \n",
    "    common_targets = set(res1.keys()).intersection(set(res2.keys()))\n",
    "    total_targets = len(common_targets)\n",
    "    \n",
    "    if total_targets == 0:\n",
    "        print(\"Error: No common targets found between runs.\")\n",
    "        return\n",
    "\n",
    "    total_overlap_percent = 0.0\n",
    "    \n",
    "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"NEAREST NEIGHBOR COMPARISON REPORT\\n\")\n",
    "        f.write(\"==================================\\n\")\n",
    "        f.write(f\"Total targets compared: {total_targets}\\n\")\n",
    "        f.write(f\"Neighbors per target: {NUM_NEIGHBORS}\\n\")\n",
    "        f.write(f\"Comparison: Drop NaN vs. Fill NaN with Median ({TARGET_FEATURE})\\n\\n\")\n",
    "        f.write(f\"{'ObjectID':<20} | {'Overlap (Cnt)':<15} | {'Overlap (%)':<15}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        for tid in common_targets:\n",
    "            set1 = set(res1[tid])\n",
    "            set2 = set(res2[tid])\n",
    "            \n",
    "            intersection = set1.intersection(set2)\n",
    "            overlap_count = len(intersection)\n",
    "            overlap_pct = (overlap_count / NUM_NEIGHBORS) * 100\n",
    "            \n",
    "            total_overlap_percent += overlap_pct\n",
    "            \n",
    "            f.write(f\"{tid:<20} | {overlap_count:<15} | {overlap_pct:<15.1f}\\n\")\n",
    "        \n",
    "        avg_overlap = total_overlap_percent / total_targets\n",
    "        \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"AVERAGE OVERLAP PERCENTAGE: {avg_overlap:.2f}%\\n\")\n",
    "        \n",
    "    print(f\"Report generated. Average overlap: {avg_overlap:.2f}%\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"ERROR: Data file '{DATA_FILE}' not found.\")\n",
    "        exit()\n",
    "\n",
    "    # 1. Load Raw Data\n",
    "    all_ids, raw_data = load_raw_data(DATA_FILE)\n",
    "    if all_ids is None:\n",
    "        exit()\n",
    "\n",
    "    # 2. Select 500 random targets (must be valid to be fair comparison)\n",
    "    target_objects = select_random_targets(all_ids, raw_data, NUM_RANDOM_TARGETS)\n",
    "\n",
    "    # 3. Run 1: Standard (Drop NaNs) -> 'without_median' (implies no imputation)\n",
    "    results_without = process_and_search(all_ids, raw_data, target_objects, mode='drop_nan')\n",
    "    \n",
    "    # 4. Save Run 1\n",
    "    with open(FILE_WITHOUT_MEDIAN, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_without, f, indent=4)\n",
    "    print(f\"Saved results to {FILE_WITHOUT_MEDIAN}\")\n",
    "\n",
    "    # 5 & 6 & 7. Run 2: Fill kurtosis_r with Median -> 'with_median'\n",
    "    results_with = process_and_search(all_ids, raw_data, target_objects, mode='fill_median')\n",
    "\n",
    "    # 8. Save Run 2\n",
    "    with open(FILE_WITH_MEDIAN, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_with, f, indent=4)\n",
    "    print(f\"Saved results to {FILE_WITH_MEDIAN}\")\n",
    "\n",
    "    # 9. Compare\n",
    "    compare_and_report(results_without, results_with, REPORT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f55c5-d1de-4adf-8f14-4decffaf1a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
